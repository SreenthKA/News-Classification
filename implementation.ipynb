{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3025,"sourceType":"datasetVersion","datasetId":1740},{"sourceId":11287999,"sourceType":"datasetVersion","datasetId":7057889}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:44:57.909071Z","iopub.execute_input":"2025-04-05T17:44:57.909522Z","iopub.status.idle":"2025-04-05T17:44:59.407739Z","shell.execute_reply.started":"2025-04-05T17:44:57.909491Z","shell.execute_reply":"2025-04-05T17:44:59.406352Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/20-newsgroups/misc.forsale.txt\n/kaggle/input/20-newsgroups/rec.autos.txt\n/kaggle/input/20-newsgroups/list.csv\n/kaggle/input/20-newsgroups/comp.os.ms-windows.misc.txt\n/kaggle/input/20-newsgroups/sci.electronics.txt\n/kaggle/input/20-newsgroups/comp.sys.mac.hardware.txt\n/kaggle/input/20-newsgroups/talk.politics.mideast.txt\n/kaggle/input/20-newsgroups/talk.politics.guns.txt\n/kaggle/input/20-newsgroups/talk.religion.misc.txt\n/kaggle/input/20-newsgroups/comp.graphics.txt\n/kaggle/input/20-newsgroups/soc.religion.christian.txt\n/kaggle/input/20-newsgroups/rec.sport.hockey.txt\n/kaggle/input/20-newsgroups/rec.sport.baseball.txt\n/kaggle/input/20-newsgroups/comp.windows.x.txt\n/kaggle/input/20-newsgroups/comp.sys.ibm.pc.hardware.txt\n/kaggle/input/20-newsgroups/rec.motorcycles.txt\n/kaggle/input/20-newsgroups/sci.med.txt\n/kaggle/input/20-newsgroups/sci.space.txt\n/kaggle/input/20-newsgroups/alt.atheism.txt\n/kaggle/input/20-newsgroups/sci.crypt.txt\n/kaggle/input/20-newsgroups/talk.politics.misc.txt\n/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\n# Load the dataset\nnewsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\nnewsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n\n# Extract features (text) and labels (categories)\nX_train, y_train = newsgroups_train.data, newsgroups_train.target\nX_test, y_test = newsgroups_test.data, newsgroups_test.target\n\n# Print some information about the dataset\nprint(\"Training set size:\", len(X_train))\nprint(\"Test set size:\", len(X_test))\nprint(\"Categories:\", newsgroups_train.target_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:44:59.409508Z","iopub.execute_input":"2025-04-05T17:44:59.410128Z","iopub.status.idle":"2025-04-05T17:45:04.077659Z","shell.execute_reply.started":"2025-04-05T17:44:59.410094Z","shell.execute_reply":"2025-04-05T17:45:04.076370Z"}},"outputs":[{"name":"stdout","text":"Training set size: 11314\nTest set size: 7532\nCategories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Define preprocessing function\ndef preprocess_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    # Join back into a string\n    return \" \".join(tokens)\n\n# Apply preprocessing to the dataset\nX_train_clean = [preprocess_text(doc) for doc in X_train]\nX_test_clean = [preprocess_text(doc) for doc in X_test]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:45:04.080523Z","iopub.execute_input":"2025-04-05T17:45:04.081318Z","iopub.status.idle":"2025-04-05T17:45:24.257582Z","shell.execute_reply.started":"2025-04-05T17:45:04.081272Z","shell.execute_reply":"2025-04-05T17:45:24.256407Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Create TF-IDF vectorizer\nvectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n\n# Fit and transform the training data\nX_train_tfidf = vectorizer.fit_transform(X_train_clean)\n\n# Transform the test data\nX_test_tfidf = vectorizer.transform(X_test_clean)\n\n# Check the shape of the resulting matrices\nprint(\"Training TF-IDF shape:\", X_train_tfidf.shape)\nprint(\"Test TF-IDF shape:\", X_test_tfidf.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:45:24.259772Z","iopub.execute_input":"2025-04-05T17:45:24.260257Z","iopub.status.idle":"2025-04-05T17:45:32.337746Z","shell.execute_reply.started":"2025-04-05T17:45:24.260211Z","shell.execute_reply":"2025-04-05T17:45:32.336169Z"}},"outputs":[{"name":"stdout","text":"Training TF-IDF shape: (11314, 5000)\nTest TF-IDF shape: (7532, 5000)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Train a Logistic Regression model\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train_tfidf, y_train)\n\n# Predict on the test set\ny_pred = clf.predict(X_test_tfidf)\n\n# Evaluate performance\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=newsgroups_train.target_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:45:32.339161Z","iopub.execute_input":"2025-04-05T17:45:32.339581Z","iopub.status.idle":"2025-04-05T17:45:35.567502Z","shell.execute_reply.started":"2025-04-05T17:45:32.339536Z","shell.execute_reply":"2025-04-05T17:45:35.566373Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.6334306956983536\n\nClassification Report:\n                           precision    recall  f1-score   support\n\n             alt.atheism       0.43      0.43      0.43       319\n           comp.graphics       0.53      0.62      0.57       389\n comp.os.ms-windows.misc       0.64      0.60      0.62       394\ncomp.sys.ibm.pc.hardware       0.63      0.59      0.61       392\n   comp.sys.mac.hardware       0.68      0.62      0.65       385\n          comp.windows.x       0.73      0.64      0.68       395\n            misc.forsale       0.72      0.74      0.73       390\n               rec.autos       0.67      0.61      0.64       396\n         rec.motorcycles       0.44      0.74      0.55       398\n      rec.sport.baseball       0.77      0.76      0.76       397\n        rec.sport.hockey       0.87      0.82      0.84       399\n               sci.crypt       0.82      0.63      0.71       396\n         sci.electronics       0.50      0.54      0.52       393\n                 sci.med       0.70      0.69      0.70       396\n               sci.space       0.69      0.70      0.70       394\n  soc.religion.christian       0.62      0.74      0.68       398\n      talk.politics.guns       0.53      0.65      0.58       364\n   talk.politics.mideast       0.80      0.68      0.74       376\n      talk.politics.misc       0.52      0.41      0.46       310\n      talk.religion.misc       0.41      0.21      0.27       251\n\n                accuracy                           0.63      7532\n               macro avg       0.64      0.62      0.62      7532\n            weighted avg       0.64      0.63      0.63      7532\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Example: Classify a new document\nnew_doc = [\"Ford Mustang 1969 breaks the record\"]\nnew_doc_clean = [preprocess_text(doc) for doc in new_doc]  # Preprocess the new document\nnew_doc_tfidf = vectorizer.transform(new_doc_clean)       # Convert to TF-IDF\npredicted_category = clf.predict(new_doc_tfidf)           # Predict category\n\nprint(\"Predicted Category:\", newsgroups_train.target_names[predicted_category[0]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:45:35.568552Z","iopub.execute_input":"2025-04-05T17:45:35.568877Z","iopub.status.idle":"2025-04-05T17:45:35.580069Z","shell.execute_reply.started":"2025-04-05T17:45:35.568850Z","shell.execute_reply":"2025-04-05T17:45:35.578803Z"}},"outputs":[{"name":"stdout","text":"Predicted Category: rec.autos\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\n# Tokenize the cleaned training data\ntokenized_train = [word_tokenize(doc) for doc in X_train_clean]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:45:35.581196Z","iopub.execute_input":"2025-04-05T17:45:35.581574Z","iopub.status.idle":"2025-04-05T17:45:41.650749Z","shell.execute_reply.started":"2025-04-05T17:45:35.581545Z","shell.execute_reply":"2025-04-05T17:45:41.649604Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\n# Train Word2Vec model\nw2v_model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=2, workers=4)\n\n# Save the model (optional)\nw2v_model.save(\"word2vec.model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:45:41.653954Z","iopub.execute_input":"2025-04-05T17:45:41.654417Z","iopub.status.idle":"2025-04-05T17:46:06.023839Z","shell.execute_reply.started":"2025-04-05T17:45:41.654365Z","shell.execute_reply":"2025-04-05T17:46:06.022616Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(\"Vocabulary size:\", len(w2v_model.wv))\nprint(\"Vector for 'space':\", w2v_model.wv['space'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:46:06.026388Z","iopub.execute_input":"2025-04-05T17:46:06.026798Z","iopub.status.idle":"2025-04-05T17:46:06.034450Z","shell.execute_reply.started":"2025-04-05T17:46:06.026761Z","shell.execute_reply":"2025-04-05T17:46:06.032904Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 37766\nVector for 'space': [-0.3371702   0.42458296  0.6542881   0.81963974  0.35731444 -0.11564608\n -0.8256642   1.6427228  -0.43374905  0.21915483 -0.37770844 -0.490688\n -0.89908856  0.44284     0.62875783 -0.05158376  0.68129224  0.3222319\n  0.26204705 -1.8111628   0.63123554 -0.03237718 -0.06418905  0.30362436\n  0.6107642   0.40036654 -0.7165361  -0.232868   -0.25195482  0.39870146\n  0.0665701  -0.90802896 -1.3272429   0.5052152  -0.33399606 -0.08364765\n -0.4275814  -0.4172086  -1.8488071  -1.2803917  -0.28706822  0.4349651\n  0.03069674  0.46281824  0.8970272   0.15681548 -0.9525128  -0.8646256\n -0.80863875 -0.09719582  0.3833773  -0.33230615 -1.3431838  -0.8684359\n -1.0022254  -0.78351927  1.685162    0.5043659  -1.0168439   0.21898514\n -0.99101126  0.6558249  -0.8908334   0.08653314 -1.9812384   1.0648988\n -0.46835923  0.61785996 -0.9427535   0.7977871  -1.8552308   0.8674219\n  0.2899699  -0.81363463 -0.32532746 -0.4354655   0.16800524 -0.01639647\n -1.1689758   0.5567187   0.07341728 -0.7560369  -0.33311585  2.177388\n -0.27697715  0.23881224 -1.4092739  -0.48977852 -0.1706055  -0.54548246\n -0.80113345  1.08788     0.79887223 -0.00354474  0.54164815  1.0833546\n  0.62721074  0.27616227  0.612756    0.5305963 ]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\n\ndef get_document_embedding(text, model):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    # Get word vectors for tokens in the model's vocabulary\n    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n    # Return the mean of the word vectors (or zeros if no valid words are found)\n    return np.mean(embeddings, axis=0) if embeddings else np.zeros(model.vector_size)\n\n# Generate document embeddings for the training and test sets\nX_train_w2v = np.array([get_document_embedding(doc, w2v_model) for doc in X_train_clean])\nX_test_w2v = np.array([get_document_embedding(doc, w2v_model) for doc in X_test_clean])\n\n# Check the shape of the resulting matrices\nprint(\"Training Word2Vec shape:\", X_train_w2v.shape)\nprint(\"Test Word2Vec shape:\", X_test_w2v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:46:06.035856Z","iopub.execute_input":"2025-04-05T17:46:06.036266Z","iopub.status.idle":"2025-04-05T17:46:21.503013Z","shell.execute_reply.started":"2025-04-05T17:46:06.036217Z","shell.execute_reply":"2025-04-05T17:46:21.501727Z"}},"outputs":[{"name":"stdout","text":"Training Word2Vec shape: (11314, 100)\nTest Word2Vec shape: (7532, 100)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Train a Logistic Regression model\nclf_w2v = LogisticRegression(max_iter=1000)\nclf_w2v.fit(X_train_w2v, y_train)\n\n# Predict on the test set\ny_pred_w2v = clf_w2v.predict(X_test_w2v)\n\n# Evaluate performance\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_w2v))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_w2v, target_names=newsgroups_train.target_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:46:21.504382Z","iopub.execute_input":"2025-04-05T17:46:21.504825Z","iopub.status.idle":"2025-04-05T17:46:24.950393Z","shell.execute_reply.started":"2025-04-05T17:46:21.504778Z","shell.execute_reply":"2025-04-05T17:46:24.948914Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.40639936271906535\n\nClassification Report:\n                           precision    recall  f1-score   support\n\n             alt.atheism       0.24      0.26      0.25       319\n           comp.graphics       0.35      0.36      0.35       389\n comp.os.ms-windows.misc       0.48      0.41      0.44       394\ncomp.sys.ibm.pc.hardware       0.38      0.39      0.39       392\n   comp.sys.mac.hardware       0.34      0.21      0.26       385\n          comp.windows.x       0.53      0.57      0.55       395\n            misc.forsale       0.54      0.62      0.58       390\n               rec.autos       0.33      0.31      0.32       396\n         rec.motorcycles       0.34      0.37      0.36       398\n      rec.sport.baseball       0.43      0.42      0.43       397\n        rec.sport.hockey       0.63      0.59      0.61       399\n               sci.crypt       0.61      0.53      0.57       396\n         sci.electronics       0.30      0.26      0.28       393\n                 sci.med       0.24      0.39      0.29       396\n               sci.space       0.41      0.44      0.42       394\n  soc.religion.christian       0.45      0.65      0.53       398\n      talk.politics.guns       0.33      0.38      0.36       364\n   talk.politics.mideast       0.47      0.60      0.53       376\n      talk.politics.misc       0.25      0.12      0.16       310\n      talk.religion.misc       0.22      0.01      0.02       251\n\n                accuracy                           0.41      7532\n               macro avg       0.39      0.39      0.38      7532\n            weighted avg       0.40      0.41      0.40      7532\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Example: Classify a new document\nnew_doc = [\"Foxtale moisturizer hydrates even dry skin\"]\nnew_doc_clean = [preprocess_text(doc) for doc in new_doc]  # Preprocess the new document\nnew_doc_w2v = np.array([get_document_embedding(doc, w2v_model) for doc in new_doc_clean])  # Convert to Word2Vec embedding\npredicted_category = clf_w2v.predict(new_doc_w2v)  # Predict category\n\nprint(\"Predicted Category:\", newsgroups_train.target_names[predicted_category[0]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:46:24.951499Z","iopub.execute_input":"2025-04-05T17:46:24.951801Z","iopub.status.idle":"2025-04-05T17:46:24.960879Z","shell.execute_reply.started":"2025-04-05T17:46:24.951776Z","shell.execute_reply":"2025-04-05T17:46:24.959726Z"}},"outputs":[{"name":"stdout","text":"Predicted Category: sci.med\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\n# Path to the pre-trained Word2Vec model\nword2vec_path = r\"/kaggle/input/word2vec/GoogleNews-vectors-negative300.bin\"\n\n# Load the pre-trained Word2Vec model\nw2v_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n\n# Test the model\nprint(\"Vector for 'space':\", w2v_model['space'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:46:24.962328Z","iopub.execute_input":"2025-04-05T17:46:24.962795Z","iopub.status.idle":"2025-04-05T17:47:26.054960Z","shell.execute_reply.started":"2025-04-05T17:46:24.962749Z","shell.execute_reply":"2025-04-05T17:47:26.053583Z"}},"outputs":[{"name":"stdout","text":"Vector for 'space': [-0.11914062  0.078125    0.05566406 -0.06542969 -0.11523438 -0.19726562\n  0.07373047 -0.03112793  0.03930664 -0.13769531 -0.04833984 -0.06542969\n -0.12304688 -0.14941406 -0.04467773 -0.06494141  0.06542969  0.11621094\n  0.15820312  0.09179688 -0.01409912 -0.14941406 -0.12255859 -0.01843262\n -0.1015625   0.29101562 -0.05297852 -0.05981445  0.0168457  -0.25195312\n -0.26171875 -0.01965332  0.03662109 -0.0007782  -0.06640625 -0.34960938\n  0.03735352  0.02502441  0.00668335 -0.07177734 -0.10546875 -0.10351562\n  0.19140625  0.4296875  -0.07080078 -0.05615234 -0.09082031  0.203125\n  0.06787109 -0.14355469  0.08935547 -0.09619141 -0.19726562 -0.16992188\n -0.18164062 -0.15332031  0.13769531 -0.08447266  0.24121094 -0.02124023\n  0.08398438 -0.13574219 -0.05834961 -0.28710938 -0.23144531  0.03759766\n -0.01611328 -0.078125   -0.11132812  0.18359375  0.03015137 -0.03466797\n  0.20019531 -0.09814453 -0.328125   -0.12792969  0.00531006  0.09570312\n -0.26171875  0.1640625   0.10449219  0.23144531  0.20898438 -0.10839844\n -0.0559082  -0.14355469  0.07128906  0.14941406  0.35546875  0.1484375\n  0.14550781 -0.21582031  0.07910156 -0.2734375   0.20605469 -0.08740234\n -0.13671875  0.24023438 -0.08154297  0.01062012 -0.00340271 -0.1171875\n -0.07177734 -0.05615234  0.0625      0.12597656  0.02648926  0.06689453\n -0.359375   -0.04370117 -0.19433594  0.06347656  0.03112793  0.171875\n -0.04785156  0.00558472  0.09814453 -0.05664062  0.265625    0.01226807\n -0.31445312 -0.2578125  -0.07958984  0.10546875 -0.05932617 -0.171875\n -0.07373047  0.20800781  0.31445312 -0.04931641  0.16894531 -0.0703125\n  0.05249023  0.125       0.20117188  0.12109375 -0.08544922  0.04345703\n -0.00219727 -0.00337219  0.16308594 -0.17285156  0.00049591 -0.00811768\n -0.14355469 -0.31445312 -0.15332031  0.06396484 -0.14160156  0.05615234\n  0.15332031  0.14648438  0.12402344  0.10400391  0.18652344  0.00872803\n  0.08740234 -0.18554688 -0.22558594 -0.17285156 -0.04541016  0.11181641\n  0.01904297 -0.1640625   0.20117188  0.00878906  0.21582031  0.25976562\n -0.22949219 -0.13964844 -0.2109375   0.01220703  0.046875    0.03076172\n  0.09472656 -0.06445312 -0.16796875 -0.20410156 -0.16113281 -0.00592041\n -0.13476562  0.03515625 -0.16796875 -0.07666016  0.27539062 -0.27929688\n  0.32421875  0.10595703  0.26171875 -0.03686523 -0.05102539 -0.17285156\n  0.0546875   0.06152344 -0.27734375  0.14160156  0.13476562  0.0534668\n -0.06738281  0.00671387 -0.13964844 -0.05371094 -0.08740234 -0.13574219\n -0.08251953 -0.21679688 -0.02856445 -0.08544922 -0.14941406  0.03466797\n -0.12353516  0.08691406 -0.20214844  0.23339844 -0.1328125   0.17089844\n  0.15429688  0.30273438 -0.14355469  0.02453613  0.06835938  0.05541992\n  0.01513672  0.00418091 -0.17675781  0.19726562 -0.11572266 -0.14648438\n  0.06103516 -0.09765625 -0.16699219 -0.25195312  0.10595703 -0.10498047\n  0.21582031 -0.01367188 -0.18359375 -0.140625   -0.08251953 -0.21679688\n  0.1875     -0.02307129  0.09082031 -0.21191406 -0.01733398 -0.02490234\n -0.00515747 -0.06640625 -0.04858398 -0.04248047 -0.0534668  -0.15332031\n -0.0111084   0.3046875   0.11181641 -0.12109375  0.18847656 -0.00601196\n -0.19335938  0.08007812 -0.15429688 -0.11035156 -0.15527344  0.16113281\n  0.18359375  0.26757812  0.10693359  0.109375   -0.09423828  0.00086594\n -0.17382812 -0.03662109  0.22265625  0.00714111 -0.01599121 -0.06396484\n -0.20117188 -0.04785156 -0.00765991 -0.17773438 -0.15234375  0.31835938\n  0.17578125  0.03564453  0.08154297  0.20703125  0.18457031 -0.02990723\n  0.00613403  0.25390625 -0.25195312  0.08447266 -0.07080078 -0.09765625\n  0.11425781 -0.06884766  0.21875    -0.08300781 -0.140625   -0.07373047]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import numpy as np\nfrom nltk.tokenize import word_tokenize\n\ndef get_document_embedding(text, model):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    # Get word vectors for tokens in the model's vocabulary\n    embeddings = [model[word] for word in tokens if word in model]\n    # Return the mean of the word vectors (or zeros if no valid words are found)\n    return np.mean(embeddings, axis=0) if embeddings else np.zeros(model.vector_size)\n\n# Generate document embeddings for the training and test sets\nX_train_w2v = np.array([get_document_embedding(doc, w2v_model) for doc in X_train_clean])\nX_test_w2v = np.array([get_document_embedding(doc, w2v_model) for doc in X_test_clean])\n\n# Check the shape of the resulting matrices\nprint(\"Training Word2Vec shape:\", X_train_w2v.shape)\nprint(\"Test Word2Vec shape:\", X_test_w2v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:47:26.056088Z","iopub.execute_input":"2025-04-05T17:47:26.056443Z","iopub.status.idle":"2025-04-05T17:47:42.337642Z","shell.execute_reply.started":"2025-04-05T17:47:26.056415Z","shell.execute_reply":"2025-04-05T17:47:42.336608Z"}},"outputs":[{"name":"stdout","text":"Training Word2Vec shape: (11314, 300)\nTest Word2Vec shape: (7532, 300)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Train a Logistic Regression model\nclf_w2v = LogisticRegression(max_iter=1000)\nclf_w2v.fit(X_train_w2v, y_train)\n\n# Predict on the test set\ny_pred_w2v = clf_w2v.predict(X_test_w2v)\n\n# Evaluate performance\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_w2v))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_w2v, target_names=newsgroups_train.target_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:47:42.338824Z","iopub.execute_input":"2025-04-05T17:47:42.339252Z","iopub.status.idle":"2025-04-05T17:47:44.671051Z","shell.execute_reply.started":"2025-04-05T17:47:42.339211Z","shell.execute_reply":"2025-04-05T17:47:44.669801Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.6103292618162507\n\nClassification Report:\n                           precision    recall  f1-score   support\n\n             alt.atheism       0.40      0.39      0.39       319\n           comp.graphics       0.56      0.58      0.57       389\n comp.os.ms-windows.misc       0.53      0.49      0.51       394\ncomp.sys.ibm.pc.hardware       0.51      0.49      0.50       392\n   comp.sys.mac.hardware       0.53      0.46      0.49       385\n          comp.windows.x       0.63      0.64      0.63       395\n            misc.forsale       0.71      0.70      0.71       390\n               rec.autos       0.46      0.69      0.55       396\n         rec.motorcycles       0.70      0.64      0.67       398\n      rec.sport.baseball       0.79      0.75      0.77       397\n        rec.sport.hockey       0.80      0.83      0.82       399\n               sci.crypt       0.66      0.63      0.65       396\n         sci.electronics       0.55      0.50      0.52       393\n                 sci.med       0.81      0.80      0.80       396\n               sci.space       0.73      0.71      0.72       394\n  soc.religion.christian       0.58      0.79      0.67       398\n      talk.politics.guns       0.50      0.63      0.56       364\n   talk.politics.mideast       0.75      0.74      0.75       376\n      talk.politics.misc       0.47      0.40      0.43       310\n      talk.religion.misc       0.32      0.07      0.12       251\n\n                accuracy                           0.61      7532\n               macro avg       0.60      0.60      0.59      7532\n            weighted avg       0.61      0.61      0.60      7532\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Example: Classify a new document\nnew_doc = [\"This is a discussion about space exploration.\"]\nnew_doc_clean = [preprocess_text(doc) for doc in new_doc]  # Preprocess the new document\nnew_doc_w2v = np.array([get_document_embedding(doc, w2v_model) for doc in new_doc_clean])  # Convert to Word2Vec embedding\npredicted_category = clf_w2v.predict(new_doc_w2v)  # Predict category\n\nprint(\"Predicted Category:\", newsgroups_train.target_names[predicted_category[0]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:47:44.672325Z","iopub.execute_input":"2025-04-05T17:47:44.672628Z","iopub.status.idle":"2025-04-05T17:47:44.681885Z","shell.execute_reply.started":"2025-04-05T17:47:44.672600Z","shell.execute_reply":"2025-04-05T17:47:44.680577Z"}},"outputs":[{"name":"stdout","text":"Predicted Category: sci.space\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!pip install transformers datasets torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:47:44.683128Z","iopub.execute_input":"2025-04-05T17:47:44.683492Z","iopub.status.idle":"2025-04-05T17:47:50.872009Z","shell.execute_reply.started":"2025-04-05T17:47:44.683449Z","shell.execute_reply":"2025-04-05T17:47:50.870473Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nimport torch\n\n# Load pre-trained BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Load pre-trained BERT model (without a classification head)\nbert_model = BertModel.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:47:50.873488Z","iopub.execute_input":"2025-04-05T17:47:50.873956Z","iopub.status.idle":"2025-04-05T17:48:09.586454Z","shell.execute_reply.started":"2025-04-05T17:47:50.873911Z","shell.execute_reply":"2025-04-05T17:48:09.585336Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def tokenize_and_encode(texts, max_length=128):\n    # Tokenize and encode the texts\n    encodings = tokenizer(\n        texts,\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'  # Return PyTorch tensors\n    )\n    return encodings\n\n# Tokenize the training and test sets\ntrain_encodings = tokenize_and_encode(X_train_clean)\ntest_encodings = tokenize_and_encode(X_test_clean)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:48:09.587579Z","iopub.execute_input":"2025-04-05T17:48:09.588332Z","iopub.status.idle":"2025-04-05T17:49:12.225159Z","shell.execute_reply.started":"2025-04-05T17:48:09.588283Z","shell.execute_reply":"2025-04-05T17:49:12.224008Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from tqdm import tqdm  # Import tqdm for progress bars\n\ndef get_bert_embeddings_in_batches(encodings, model, batch_size=32):\n    # Move inputs to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    # Prepare inputs\n    input_ids = encodings['input_ids']\n    attention_mask = encodings['attention_mask']\n    \n    # Initialize a list to store embeddings\n    all_embeddings = []\n    \n    # Calculate the total number of batches\n    num_batches = (len(input_ids) + batch_size - 1) // batch_size\n    \n    # Process in batches with a progress bar\n    for i in tqdm(range(0, len(input_ids), batch_size), total=num_batches, desc=\"Processing batches\"):\n        # Extract the current batch\n        batch_input_ids = input_ids[i:i + batch_size].to(device)\n        batch_attention_mask = attention_mask[i:i + batch_size].to(device)\n        \n        # Generate embeddings\n        with torch.no_grad():\n            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n        \n        # Extract [CLS] token embeddings (first token in each sequence)\n        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        all_embeddings.append(cls_embeddings)\n    \n    # Concatenate all embeddings into a single array\n    return np.concatenate(all_embeddings, axis=0)\n\n# Generate embeddings for the training and test sets\nX_train_bert = get_bert_embeddings_in_batches(train_encodings, bert_model, batch_size=32)\nX_test_bert = get_bert_embeddings_in_batches(test_encodings, bert_model, batch_size=32)\n\n# Check the shape of the resulting matrices\nprint(\"Training BERT shape:\", X_train_bert.shape)\nprint(\"Test BERT shape:\", X_test_bert.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:52:58.722114Z","iopub.execute_input":"2025-04-05T17:52:58.722593Z","iopub.status.idle":"2025-04-05T18:12:37.195469Z","shell.execute_reply.started":"2025-04-05T17:52:58.722550Z","shell.execute_reply":"2025-04-05T18:12:37.192893Z"}},"outputs":[{"name":"stderr","text":"Processing batches:  46%|████▌     | 163/354 [19:38<23:00,  7.23s/it] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-d2b49441c4a4>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Generate embeddings for the training and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mX_train_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bert_embeddings_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mX_test_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bert_embeddings_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-d2b49441c4a4>\u001b[0m in \u001b[0;36mget_bert_embeddings_in_batches\u001b[0;34m(encodings, model, batch_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Generate embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Extract [CLS] token embeddings (first token in each sequence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Train a Logistic Regression model\nclf_bert = LogisticRegression(max_iter=1000)\nclf_bert.fit(X_train_bert, y_train)\n\n# Predict on the test set\ny_pred_bert = clf_bert.predict(X_test_bert)\n\n# Evaluate performance\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_bert))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_bert, target_names=newsgroups_train.target_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:51:40.941139Z","iopub.status.idle":"2025-04-05T17:51:40.941551Z","shell.execute_reply":"2025-04-05T17:51:40.941412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Classify a new document\nnew_doc = [\"This is a discussion about space exploration.\"]\nnew_doc_clean = [preprocess_text(doc) for doc in new_doc]  # Preprocess the new document\nnew_doc_encodings = tokenize_and_encode(new_doc_clean)    # Tokenize the new document\nnew_doc_bert = get_bert_embeddings(new_doc_encodings, bert_model)  # Convert to BERT embedding\npredicted_category = clf_bert.predict(new_doc_bert)  # Predict category\n\nprint(\"Predicted Category:\", newsgroups_train.target_names[predicted_category[0]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T17:51:40.942345Z","iopub.status.idle":"2025-04-05T17:51:40.942667Z","shell.execute_reply":"2025-04-05T17:51:40.942542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}