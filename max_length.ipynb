{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd30cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sreen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sreen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n",
      "Preprocessing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6872/6872 [05:48<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained BERT model...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "df = pd.read_csv(r'E:\\SNU Chennai\\projects\\NLP project\\NEWS classification\\news article\\news-article-categories.csv')\n",
    "\n",
    "# Combine 'title' and 'body' columns into a single 'text' column\n",
    "df['text'] = df['title'] + \" \" + df['body']\n",
    "\n",
    "# Drop rows where 'text' is missing\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Preprocessing function using spaCy\n",
    "def preprocess_text(text):\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    # Remove stopwords and non-alphabetic tokens, then lemmatize\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    # Join back into a string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'text' column with tqdm\n",
    "print(\"Preprocessing text...\")\n",
    "tqdm.pandas()  # Enable tqdm progress_apply\n",
    "df['clean_text'] = df['text'].progress_apply(preprocess_text)  # Use progress_apply instead of apply\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['category_encoded'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "print(\"Loading pre-trained BERT model...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8855b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84123ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6872/6872 [00:19<00:00, 360.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Max token length: 5515\n",
      "ðŸ“ˆ 95th percentile token length: 881.4499999999998\n",
      "ðŸ“‰ Average token length: 363.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to count tokens in a single row of clean_text\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "# Apply to your preprocessed text\n",
    "print(\"Calculating token lengths...\")\n",
    "df['token_count'] = df['clean_text'].progress_apply(count_tokens)\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\nðŸ“Š Max token length: {df['token_count'].max()}\")\n",
    "print(f\"ðŸ“ˆ 95th percentile token length: {df['token_count'].quantile(0.95)}\")\n",
    "print(f\"ðŸ“‰ Average token length: {df['token_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2d0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preprocessing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6872/6872 [06:17<00:00, 18.20it/s]\n",
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [02:56<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.4524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [03:14<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.6357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [03:07<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.1939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [02:59<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [05:09<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.9274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [01:03<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.7687\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "ARTS & CULTURE       0.86      0.87      0.86       205\n",
      "      BUSINESS       0.70      0.65      0.67       114\n",
      "        COMEDY       0.76      0.80      0.78        74\n",
      "         CRIME       0.76      0.82      0.79        57\n",
      "     EDUCATION       0.88      0.79      0.83       108\n",
      " ENTERTAINMENT       0.81      0.75      0.78       100\n",
      "   ENVIRONMENT       0.72      0.75      0.73        97\n",
      "         MEDIA       0.61      0.70      0.65        66\n",
      "      POLITICS       0.80      0.63      0.71       103\n",
      "      RELIGION       0.83      0.90      0.86       101\n",
      "       SCIENCE       0.74      0.73      0.73        55\n",
      "        SPORTS       0.83      0.95      0.88       101\n",
      "          TECH       0.67      0.79      0.72        84\n",
      "         WOMEN       0.67      0.56      0.61       110\n",
      "\n",
      "      accuracy                           0.77      1375\n",
      "     macro avg       0.76      0.76      0.76      1375\n",
      "  weighted avg       0.77      0.77      0.77      1375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r'E:\\SNU Chennai\\projects\\NLP project\\NEWS classification\\news article\\news-article-categories.csv')\n",
    "df['text'] = df['title'] + \" \" + df['body']\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Text preprocessing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n",
    "\n",
    "print(\"Preprocessing text...\")\n",
    "tqdm.pandas()\n",
    "df['clean_text'] = df['text'].progress_apply(preprocess_text)\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['category_encoded'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['clean_text'], df['category_encoded'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "def encode_text(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "train_encodings = encode_text(train_texts)\n",
    "val_encodings = encode_text(val_texts)\n",
    "\n",
    "train_labels = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "val_labels = torch.tensor(val_labels.values, dtype=torch.long)\n",
    "\n",
    "# Dataset\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "val_dataset = NewsDataset(val_encodings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Model\n",
    "class BERTBiLSTM(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BERTBiLSTM, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.bilstm = nn.LSTM(768, 64, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_output = outputs.last_hidden_state\n",
    "        lstm_out, _ = self.bilstm(bert_output)\n",
    "        final_feature = lstm_out[:, -1, :]\n",
    "        out = self.dropout(final_feature)\n",
    "        return self.fc(out)\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = BERTBiLSTM(bert_model, num_classes).to(device)\n",
    "\n",
    "# Unfreeze final BERT layer and pooler\n",
    "for name, param in model.bert.named_parameters():\n",
    "    \n",
    "    if 'layer.11' in name or 'pooler' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Training and evaluation\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_model(model, loader, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=label_encoder.classes_)\n",
    "    return acc, report\n",
    "\n",
    "# Run training and evaluation\n",
    "train_model(model, train_loader, epochs=5)\n",
    "acc, report = evaluate_model(model, val_loader)\n",
    "print(f\"\\nValidation Accuracy: {acc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7060dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking Misclassifications: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86/86 [01:09<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total misclassifications: 318\n",
      "\n",
      "Sample 1:\n",
      "Text       : johns chair board woman catalyst president ceo deborah gillis arrive london year ago astonish find have animate conversation male custom officer woman board unlike united states deborah delighted issu...\n",
      "Actual     : WOMEN\n",
      "Predicted  : BUSINESS\n",
      "\n",
      "Sample 2:\n",
      "Text       : late paycheck mess life sharisse tracey know deep level family live paycheck paycheck check come pay bill saving speak easy ignore reality check post regularly fully understand live paycheck paycheck ...\n",
      "Actual     : WOMEN\n",
      "Predicted  : EDUCATION\n",
      "\n",
      "Sample 3:\n",
      "Text       : asian team participate global robotic challenge xinxin zhang research intern east west center washington graduate student public policy university chicago note article originally appear east west cent...\n",
      "Actual     : WOMEN\n",
      "Predicted  : EDUCATION\n",
      "\n",
      "Sample 4:\n",
      "Text       : charge assumption race gender close colleague observe college head student display behavior disrespectful likely different dare leader different identity asian american male tempt point phenomenon rem...\n",
      "Actual     : EDUCATION\n",
      "Predicted  : WOMEN\n",
      "\n",
      "Sample 5:\n",
      "Text       : good leader good interface good graphical user interface gui computer code make device app relatable shock good leader quality work hard relate positively bring good think good leader have kind excell...\n",
      "Actual     : BUSINESS\n",
      "Predicted  : TECH\n",
      "\n",
      "Sample 6:\n",
      "Text       : winter olympic rescue kitten puppy melt cold heart rescue kitten puppy get spirit winter olympic north carolina bid find forever home adoptable animal charlotte mecklenburg animal care control host co...\n",
      "Actual     : SPORTS\n",
      "Predicted  : ENVIRONMENT\n",
      "\n",
      "Sample 7:\n",
      "Text       : nobel win economist richard thaler jovial professor richard thaler university chicago receive news win nobel memorial prize economic science contribution behavioral economic face eager press unusual m...\n",
      "Actual     : BUSINESS\n",
      "Predicted  : SCIENCE\n",
      "\n",
      "Sample 8:\n",
      "Text       : gop lawmaker get sunday school botch religious reference republican congressman call thousand miss fbi text message great coincidence immaculate conception struggle explain mean fbi say message lose t...\n",
      "Actual     : MEDIA\n",
      "Predicted  : RELIGION\n",
      "\n",
      "Sample 9:\n",
      "Text       : eve ensler want topple patriarchy revolutionary love sick tired rose chocolate valentine day eve ensler billion rise campaign alternative v day event strike fancy come feb billion rise opt card candy ...\n",
      "Actual     : ARTS & CULTURE\n",
      "Predicted  : WOMEN\n",
      "\n",
      "Sample 10:\n",
      "Text       : totally nail like crush stranger imagine future cute barista know charle depressingly hilarious web series charles way create charles gould daniel hurwitz new york comedy live apt series chronicle man...\n",
      "Actual     : COMEDY\n",
      "Predicted  : ENTERTAINMENT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert back encoded labels to original category names\n",
    "inv_label_map = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "\n",
    "# Add this function\n",
    "def print_misclassifications(model, loader, texts, true_labels):\n",
    "    model.eval()\n",
    "    misclassified = []\n",
    "    index = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Checking Misclassifications\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            for i in range(len(preds)):\n",
    "                true_label = labels[i].item()\n",
    "                pred_label = preds[i].item()\n",
    "                if true_label != pred_label:\n",
    "                    misclassified.append({\n",
    "                        'text': texts.iloc[index],\n",
    "                        'actual': inv_label_map[true_label],\n",
    "                        'predicted': inv_label_map[pred_label]\n",
    "                    })\n",
    "                index += 1\n",
    "\n",
    "    # Print first few misclassified examples\n",
    "    print(f\"\\nTotal misclassifications: {len(misclassified)}\\n\")\n",
    "    for i, sample in enumerate(misclassified[:10]):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"Text       : {sample['text'][:200]}...\")\n",
    "        print(f\"Actual     : {sample['actual']}\")\n",
    "        print(f\"Predicted  : {sample['predicted']}\\n\")\n",
    "\n",
    "# Call this after evaluation\n",
    "print_misclassifications(model, val_loader, val_texts.reset_index(drop=True), val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ea6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
