{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3025,"sourceType":"datasetVersion","datasetId":1740},{"sourceId":11294794,"sourceType":"datasetVersion","datasetId":7062468}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:21:23.733876Z","iopub.execute_input":"2025-04-06T08:21:23.734074Z","iopub.status.idle":"2025-04-06T08:21:25.881041Z","shell.execute_reply.started":"2025-04-06T08:21:23.734055Z","shell.execute_reply":"2025-04-06T08:21:25.880058Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/20-newsgroups/misc.forsale.txt\n/kaggle/input/20-newsgroups/rec.autos.txt\n/kaggle/input/20-newsgroups/list.csv\n/kaggle/input/20-newsgroups/comp.os.ms-windows.misc.txt\n/kaggle/input/20-newsgroups/sci.electronics.txt\n/kaggle/input/20-newsgroups/comp.sys.mac.hardware.txt\n/kaggle/input/20-newsgroups/talk.politics.mideast.txt\n/kaggle/input/20-newsgroups/talk.politics.guns.txt\n/kaggle/input/20-newsgroups/talk.religion.misc.txt\n/kaggle/input/20-newsgroups/comp.graphics.txt\n/kaggle/input/20-newsgroups/soc.religion.christian.txt\n/kaggle/input/20-newsgroups/rec.sport.hockey.txt\n/kaggle/input/20-newsgroups/rec.sport.baseball.txt\n/kaggle/input/20-newsgroups/comp.windows.x.txt\n/kaggle/input/20-newsgroups/comp.sys.ibm.pc.hardware.txt\n/kaggle/input/20-newsgroups/rec.motorcycles.txt\n/kaggle/input/20-newsgroups/sci.med.txt\n/kaggle/input/20-newsgroups/sci.space.txt\n/kaggle/input/20-newsgroups/alt.atheism.txt\n/kaggle/input/20-newsgroups/sci.crypt.txt\n/kaggle/input/20-newsgroups/talk.politics.misc.txt\n/kaggle/input/word2vec-pretrained/GoogleNews-vectors-negative300.bin\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom collections import Counter\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split  # Add this import\n\n# Download NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Define readable category names\nreadable_category_names = {\n    'alt.atheism': 'Atheism',\n    'comp.graphics': 'Graphics',\n    'comp.os.ms-windows.misc': 'Windows',\n    'comp.sys.ibm.pc.hardware': 'PC Hardware',\n    'comp.sys.mac.hardware': 'Mac Hardware',\n    'comp.windows.x': 'X Windows',\n    'misc.forsale': 'For Sale',\n    'rec.autos': 'Autos',\n    'rec.motorcycles': 'Motorcycles',\n    'rec.sport.baseball': 'Baseball',\n    'rec.sport.hockey': 'Hockey',\n    'sci.crypt': 'Cryptography',\n    'sci.electronics': 'Electronics',\n    'sci.med': 'Medicine',\n    'sci.space': 'Space',\n    'soc.religion.christian': 'Christianity',\n    'talk.politics.guns': 'Guns',\n    'talk.politics.mideast': 'Middle East Politics',\n    'talk.politics.misc': 'General Politics',\n    'talk.religion.misc': 'Religion'\n}\n\n# Load the dataset\nnewsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n\n# Extract features (text) and labels (categories)\nX, y = newsgroups.data, newsgroups.target\n\n# Map target names to readable names\nreadable_target_names = [readable_category_names[name] for name in newsgroups.target_names]\n\n# Split the dataset into 80% training and 20% test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Print dataset information\nprint(\"Training set size:\", len(X_train))\nprint(\"Test set size:\", len(X_test))\nprint(\"Categories:\", readable_target_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:21:44.499263Z","iopub.execute_input":"2025-04-06T08:21:44.499812Z","iopub.status.idle":"2025-04-06T08:22:14.089392Z","shell.execute_reply.started":"2025-04-06T08:21:44.499762Z","shell.execute_reply":"2025-04-06T08:22:14.088490Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\nTraining set size: 9051\nTest set size: 2263\nCategories: ['Atheism', 'Graphics', 'Windows', 'PC Hardware', 'Mac Hardware', 'X Windows', 'For Sale', 'Autos', 'Motorcycles', 'Baseball', 'Hockey', 'Cryptography', 'Electronics', 'Medicine', 'Space', 'Christianity', 'Guns', 'Middle East Politics', 'General Politics', 'Religion']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Preprocessing function\ndef preprocess_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove special characters and numbers\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    # Join back into a string\n    return \" \".join(tokens)\n\n# Apply preprocessing to the dataset\nX_train_clean = [preprocess_text(doc) for doc in X_train]\nX_test_clean = [preprocess_text(doc) for doc in X_test]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:22:29.911724Z","iopub.execute_input":"2025-04-06T08:22:29.912046Z","iopub.status.idle":"2025-04-06T08:22:39.711739Z","shell.execute_reply.started":"2025-04-06T08:22:29.912018Z","shell.execute_reply":"2025-04-06T08:22:39.711032Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Count the number of news articles in each category\ntrain_category_counts = Counter([readable_target_names[label] for label in y_train])\ntest_category_counts = Counter([readable_target_names[label] for label in y_test])\n\nprint(\"\\nTraining Set Category Distribution:\")\nfor category, count in train_category_counts.items():\n    print(f\"{category}: {count}\")\n\nprint(\"\\nTest Set Category Distribution:\")\nfor category, count in test_category_counts.items():\n    print(f\"{category}: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:22:39.712831Z","iopub.execute_input":"2025-04-06T08:22:39.713077Z","iopub.status.idle":"2025-04-06T08:22:39.727695Z","shell.execute_reply.started":"2025-04-06T08:22:39.713055Z","shell.execute_reply":"2025-04-06T08:22:39.724897Z"}},"outputs":[{"name":"stdout","text":"\nTraining Set Category Distribution:\nMedicine: 475\nAutos: 475\nMotorcycles: 478\nElectronics: 473\nMiddle East Politics: 451\nPC Hardware: 472\nWindows: 473\nGeneral Politics: 372\nMac Hardware: 463\nBaseball: 478\nCryptography: 476\nSpace: 474\nGraphics: 467\nAtheism: 384\nChristianity: 479\nHockey: 480\nX Windows: 474\nReligion: 302\nGuns: 437\nFor Sale: 468\n\nTest Set Category Distribution:\nX Windows: 119\nMac Hardware: 115\nGraphics: 117\nPC Hardware: 118\nCryptography: 119\nAutos: 119\nMiddle East Politics: 113\nFor Sale: 117\nChristianity: 120\nSpace: 119\nReligion: 75\nGuns: 109\nWindows: 118\nMotorcycles: 120\nElectronics: 118\nHockey: 120\nMedicine: 119\nBaseball: 119\nAtheism: 96\nGeneral Politics: 93\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom gensim.models import KeyedVectors  # For loading pretrained Word2Vec\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\n# Download NLTK resources\nnltk.download('punkt')\n\n# Load pretrained Word2Vec model\nprint(\"Loading pretrained Word2Vec model...\")\nword2vec_model = KeyedVectors.load_word2vec_format(\n    '/kaggle/input/word2vec-pretrained/GoogleNews-vectors-negative300.bin', binary=True\n)\nprint(\"Pretrained Word2Vec model loaded.\")\n\n# Function to generate document embeddings using Word2Vec\ndef get_document_embedding(text, model, vector_size=300):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    # Get word vectors for tokens in the model's vocabulary\n    embeddings = [model[word] for word in tokens if word in model]\n    # Return the mean of the word vectors (or zeros if no valid words are found)\n    return np.mean(embeddings, axis=0) if embeddings else np.zeros(vector_size)\n\n# Generate document embeddings for the training and test sets\nprint(\"Generating document embeddings for the training set...\")\nX_train_w2v = np.array([get_document_embedding(doc, word2vec_model) for doc in X_train_clean])\n\nprint(\"Generating document embeddings for the test set...\")\nX_test_w2v = np.array([get_document_embedding(doc, word2vec_model) for doc in X_test_clean])\n\n# Check the shape of the resulting matrices\nprint(\"\\nTraining Word2Vec Embeddings Shape:\", X_train_w2v.shape)\nprint(\"Test Word2Vec Embeddings Shape:\", X_test_w2v.shape)\n\n# Train a Logistic Regression model on Word2Vec embeddings\nprint(\"\\nTraining Logistic Regression on Word2Vec embeddings...\")\nclf_w2v = LogisticRegression(max_iter=1000, random_state=42)\nclf_w2v.fit(X_train_w2v, y_train)\n\n# Predict on the test set\ny_pred_w2v = clf_w2v.predict(X_test_w2v)\n\n# Evaluate performance\nprint(\"\\nAccuracy with Word2Vec:\", accuracy_score(y_test, y_pred_w2v))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_w2v, target_names=readable_target_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:23:04.311790Z","iopub.execute_input":"2025-04-06T08:23:04.312076Z","iopub.status.idle":"2025-04-06T08:24:01.302306Z","shell.execute_reply.started":"2025-04-06T08:23:04.312054Z","shell.execute_reply":"2025-04-06T08:24:01.301468Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nLoading pretrained Word2Vec model...\nPretrained Word2Vec model loaded.\nGenerating document embeddings for the training set...\nGenerating document embeddings for the test set...\n\nTraining Word2Vec Embeddings Shape: (9051, 300)\nTest Word2Vec Embeddings Shape: (2263, 300)\n\nTraining Logistic Regression on Word2Vec embeddings...\n\nAccuracy with Word2Vec: 0.6429518338488732\n\nClassification Report:\n                       precision    recall  f1-score   support\n\n             Atheism       0.53      0.39      0.45        96\n            Graphics       0.57      0.56      0.56       117\n             Windows       0.58      0.50      0.54       118\n         PC Hardware       0.52      0.58      0.54       118\n        Mac Hardware       0.56      0.46      0.50       115\n           X Windows       0.61      0.61      0.61       119\n            For Sale       0.61      0.63      0.62       117\n               Autos       0.46      0.74      0.57       119\n         Motorcycles       0.74      0.69      0.72       120\n            Baseball       0.74      0.80      0.77       119\n              Hockey       0.81      0.78      0.79       120\n        Cryptography       0.76      0.68      0.72       119\n         Electronics       0.72      0.66      0.69       118\n            Medicine       0.81      0.85      0.83       119\n               Space       0.74      0.72      0.73       119\n        Christianity       0.60      0.84      0.70       120\n                Guns       0.63      0.63      0.63       109\nMiddle East Politics       0.76      0.81      0.79       113\n    General Politics       0.59      0.61      0.60        93\n            Religion       0.18      0.03      0.05        75\n\n            accuracy                           0.64      2263\n           macro avg       0.63      0.63      0.62      2263\n        weighted avg       0.64      0.64      0.63      2263\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Identify misclassifications\nmisclassifications = []\nfor i in range(len(X_test)):\n    if y_pred_w2v[i] != y_test[i]:  # Check if prediction is incorrect\n        misclassifications.append({\n            \"Input\": X_test[i],  # Original input text\n            \"Predicted\": readable_target_names[y_pred_w2v[i]],  # Predicted category\n            \"Correct\": readable_target_names[y_test[i]]  # Correct category\n        })\n\n# Print misclassifications\nprint(\"\\nMisclassifications:\")\nfor idx, misclassification in enumerate(misclassifications[:5]):\n    print(f\"Example {idx + 1}:\")\n    print(f\"  Input: {misclassification['Input']}\")\n    print(f\"  Predicted: {misclassification['Predicted']}\")\n    print(f\"  Correct: {misclassification['Correct']}\")\n    print(\"-\" * 80)\n\n# Total number of misclassifications\nprint(f\"Total Misclassifications: {len(misclassifications)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:25:32.420620Z","iopub.execute_input":"2025-04-06T08:25:32.420943Z","iopub.status.idle":"2025-04-06T08:25:32.433155Z","shell.execute_reply.started":"2025-04-06T08:25:32.420918Z","shell.execute_reply":"2025-04-06T08:25:32.432110Z"}},"outputs":[{"name":"stdout","text":"\nMisclassifications:\nExample 1:\n  Input: After reading reports from Germany of success in accelerating a Quadra\nor Centris simply by changing the clock oscillator, I decided to test the\nclaim. I pulled out my Variable Speed Overdrive and the motherboard's\n50 mhz clock chip. I put a socket in the clock's place and inserted a\n64 mhz TTL clock oscillator I had left over from working on some SI's.\nI can't believe it. It actually works. I'm not getting SCSI timing errors\neither. This is only after a short run time but I'll keep posting results.\nDid I spend all that money on the VSO for nothing? If this keeps working,\nthe lack of a double boot in itself will be worth the effort.\n  Predicted: PC Hardware\n  Correct: Mac Hardware\n--------------------------------------------------------------------------------\nExample 2:\n  Input: : |> \n: |>         Is there any way to connect two pointing devices to one serial\n: |>         port? I haven't tried this but I believe they would interfere\n: |>         with each other (?) even if only one at a time would be used.\n\n: \tJust get an A-B switch for RS232. Look in Computer Shopper.\n: They are available fairly cheap. They allow switching between two\n: serial devices on a single port.\n\nUnfortunately the poster wants to use an internal and an external modem so a\nswitch isn't going to help them.  If you aren't using your com ports for\nanything else, just define them on different com ports.  Define your internal\nmodem to be say, com1, and your external modem to be com3.  You really\nshouldn't have to worry about interrupt conflicts since you won't be using\nboth modems at the same time :).\n\n  Predicted: PC Hardware\n  Correct: Graphics\n--------------------------------------------------------------------------------\nExample 3:\n  Input: For those of you who couldn't find X-Appeal, it is availible at\nthe following sitex:\n\n\tascwide.ascii.co.jp in the /pub/MSDOS/xappeal dir\n\twuarchive.wustl.edu in the /mirrors4/garbo.uwasa.fi/demo\n\tdirectory\n\n\tThe three files are xap13exe.aip, xap10fon.zip and\n   drivers.zip.\n\nJosh\n  Predicted: Windows\n  Correct: X Windows\n--------------------------------------------------------------------------------\nExample 4:\n  Input: \n    ^^^\nHow long does he have to take in fixing it?  Does he have to use new\nparts when he repairs it or can he substitute used parts without your\nknowledge?  Can he charge you for repairs that should be under warranty\nbut he claims are due to improper maintenance on your part? \n\nWhen it comes to local dealers:\n\n- Have fun getting consistently good support.  Most of their \"techs\" are\nre-treaded salesmen, not trained technicians, with a high turnover rate.\n\n- Have fun getting in-warranty work done quickly and courteously.\n\n- Have fun getting out-of-warranty work done cheaply, or even done\nperiod, unless you are on a paid service contract.\n\nHaving been both a service technician, and a service manager, at a\nComputerLand franchise and another retail computer place, I know what\nI'm talking about. \n\nI also know the \"local service\" scam that retail computer dealers like\nto push when they're selling.  It's that same old song that car dealers\nhaving been singing for years -- \"Buy from me and you'll get good\nservice. We always treat our customers right!  Buy from my competition\nand you'll be sorry if you need service.\" \n\nExperienced mail order buyers know that there are some mail order\ncompanies that give excellent service, including overnight replacement parts, \non-site calls, etc. There are probably some local dealers that can give you\ngood service, too. But if you think all local dealers give consistenly good \nservice, you are wrong.  I have many anectdotes to prove my point, \nbut I'm sure there are others on the net can do a better job than I can.\n  Predicted: Autos\n  Correct: PC Hardware\n--------------------------------------------------------------------------------\nExample 5:\n  Input: \n\nThere is a certain truth to this statement. Only I would use the word\n\"medicine\" instead of drug.  With regard to the condition of the human\nsoul, Christianity is first and foremost a healing medicine.  It also\nstrengthens and enables one, as healing takes hold, to grow in new\nstrength and health to live and be and to do that for which God created \nus.\n\n\nChrist's medicine, rightly allowed to work, brings one nearer to\nreality and offers the clarity of understanding and the strength\nof spirit with which to meet it in a healthy human way.\n\n\n(small spelling correction added)\n  Predicted: Medicine\n  Correct: Christianity\n--------------------------------------------------------------------------------\nTotal Misclassifications: 808\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### **Analysis of Misclassifications**\n\n#### **1. Common Patterns in Misclassifications**\nFrom the examples provided, we can observe a few recurring themes:\n\n1. **Overlapping Themes Between Categories**:\n   - Example 1: \"Mac Hardware\" vs. \"PC Hardware\"\n     - The text discusses hardware components (e.g., clock oscillator), which are relevant to both Mac and PC hardware. However, the mention of \"Quadra\" or \"Centris\" (Mac-specific models) should have tipped the model toward \"Mac Hardware.\"\n   - Example 4: \"PC Hardware\" vs. \"Autos\"\n     - The text discusses technical support and repairs for computers, but the model incorrectly predicts \"Autos.\" This suggests that the embeddings failed to capture domain-specific terms like \"techs,\" \"warranty,\" or \"computer.\"\n\n2. **Ambiguous Language**:\n   - Example 5: \"Christianity\" vs. \"Medicine\"\n     - The text uses metaphorical language (\"medicine for the soul\") to describe Christianity. While the word \"medicine\" is prominent, the context clearly relates to religion. The model likely focused too much on the literal term \"medicine\" rather than the broader context.\n\n3. **Rare or Domain-Specific Terms**:\n   - Example 3: \"X Windows\" vs. \"Windows\"\n     - The text mentions \"X-Appeal\" and \"X Windows\"-specific directories (e.g., `/pub/MSDOS/xappeal`). However, the model incorrectly predicts \"Windows,\" likely because it doesn’t recognize \"X Windows\" as a distinct category.\n   - Example 2: \"Graphics\" vs. \"PC Hardware\"\n     - The text discusses serial ports and modems, which are more closely related to hardware than graphics. However, the model predicts \"Graphics,\" possibly due to insufficient training on hardware-related terms.\n\n4. **Lack of Contextual Understanding**:\n   - Word2Vec embeddings capture semantic relationships between words but lack contextual understanding. For example:\n     - In Example 5, the model fails to interpret the metaphorical use of \"medicine.\"\n     - In Example 3, the model doesn’t recognize \"X Windows\" as a specific operating system.","metadata":{}},{"cell_type":"markdown","source":"### **Why Are These Errors Happening?**\n\n1. **Out-of-Vocabulary (OOV) Words**:\n   - Pretrained Word2Vec embeddings may not include domain-specific terms like \"Centris,\" \"Quadra,\" or \"X-Appeal.\" Documents containing such terms may result in zero vectors, leading to poor classification.\n\n2. **Averaging Embeddings**:\n   - The document embedding is computed as the average of word vectors, which can dilute important signals. For example, in Example 5, the word \"medicine\" dominates the embedding, overshadowing the religious context.\n\n3. **Overlap Between Categories**:\n   - Some categories (e.g., \"PC Hardware\" and \"Mac Hardware\") share similar vocabulary, making it harder for the model to distinguish between them.\n\n4. **Lack of Contextual Information**:\n   - Word2Vec embeddings don’t capture the order or context of words, which can lead to misinterpretations. For example, the metaphorical use of \"medicine\" in Example 5 is lost.","metadata":{}},{"cell_type":"markdown","source":"### **How Can We Improve the Model?**\n\n#### **2. Fine-Tune Word2Vec Embeddings**\n- Fine-tune the pretrained Word2Vec model on your dataset to include domain-specific terms (e.g., \"X Windows,\" \"Centris\").\n- This will help the model better represent rare or specialized vocabulary.\n\n#### **3. Weighted Averaging**\n- Instead of averaging all word vectors equally, weight them by their TF-IDF scores. This ensures that important words (e.g., \"Centris\" in Example 1) contribute more to the document embedding.\n\n#### **4. Group Similar Categories**\n- Combine categories with overlapping themes (e.g., \"PC Hardware\" and \"Mac Hardware\" → \"Computers & Technology\"). This reduces ambiguity and improves classification accuracy.\n---\n\n### **Key Takeaways**\n1. **Strengths of Word2Vec**:\n   - Captures semantic relationships between words (e.g., \"car\" and \"vehicle\").\n   - Works well for tasks where word similarity matters.\n\n2. **Limitations of Word2Vec**:\n   - Struggles with domain-specific terms and OOV words.\n   - Lacks contextual understanding, leading to errors in ambiguous cases.","metadata":{}},{"cell_type":"markdown","source":"Weighted Averaging\nInstead of averaging all word vectors equally, weight them by their TF-IDF scores. This ensures that important words (e.g., \"Centris\" in Example 1) contribute more to the document embedding.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom gensim.models import KeyedVectors  # For loading pretrained Word2Vec\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\n\n# Download NLTK resources\nnltk.download('punkt')\n\n# Load pretrained Word2Vec model\nprint(\"Loading pretrained Word2Vec model...\")\nword2vec_model = KeyedVectors.load_word2vec_format(\n    '/kaggle/input/word2vec-pretrained/GoogleNews-vectors-negative300.bin', binary=True\n)\nprint(\"Pretrained Word2Vec model loaded.\")\n\n# Create TF-IDF vectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\n\n# Fit and transform the training data to get TF-IDF scores\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train_clean)\n\n# Transform the test data\nX_test_tfidf = tfidf_vectorizer.transform(X_test_clean)\n\n# Get the vocabulary from the TF-IDF vectorizer\nvocabulary = tfidf_vectorizer.get_feature_names_out()\n\n# Function to generate weighted document embeddings using Word2Vec + TF-IDF\ndef get_weighted_document_embedding(text, model, tfidf_vectorizer, vector_size=300):\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    # Get the TF-IDF scores for the document\n    tfidf_scores = dict(zip(vocabulary, tfidf_vectorizer.transform([text]).toarray()[0]))\n    # Initialize variables for weighted sum and total weight\n    weighted_sum = np.zeros(vector_size)\n    total_weight = 0\n    # Iterate over tokens and compute the weighted sum\n    for word in tokens:\n        if word in model and word in tfidf_scores:\n            weight = tfidf_scores[word]\n            weighted_sum += weight * model[word]\n            total_weight += weight\n    # Return the weighted average (or zeros if no valid words are found)\n    return weighted_sum / total_weight if total_weight > 0 else np.zeros(vector_size)\n\n# Generate weighted document embeddings for the training and test sets\nprint(\"Generating weighted document embeddings for the training set...\")\nX_train_w2v_tfidf = np.array([get_weighted_document_embedding(doc, word2vec_model, tfidf_vectorizer) for doc in X_train_clean])\n\nprint(\"Generating weighted document embeddings for the test set...\")\nX_test_w2v_tfidf = np.array([get_weighted_document_embedding(doc, word2vec_model, tfidf_vectorizer) for doc in X_test_clean])\n\n# Check the shape of the resulting matrices\nprint(\"\\nTraining Weighted Word2Vec Embeddings Shape:\", X_train_w2v_tfidf.shape)\nprint(\"Test Weighted Word2Vec Embeddings Shape:\", X_test_w2v_tfidf.shape)\n\n# Train a Logistic Regression model on weighted Word2Vec embeddings\nprint(\"\\nTraining Logistic Regression on Weighted Word2Vec embeddings...\")\nclf_w2v_tfidf = LogisticRegression(max_iter=1000, random_state=42)\nclf_w2v_tfidf.fit(X_train_w2v_tfidf, y_train)\n\n# Predict on the test set\ny_pred_w2v_tfidf = clf_w2v_tfidf.predict(X_test_w2v_tfidf)\n\n# Evaluate performance\nprint(\"\\nAccuracy with Weighted Word2Vec:\", accuracy_score(y_test, y_pred_w2v_tfidf))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_w2v_tfidf, target_names=readable_target_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T08:32:39.751841Z","iopub.execute_input":"2025-04-06T08:32:39.752172Z","iopub.status.idle":"2025-04-06T08:33:34.869829Z","shell.execute_reply.started":"2025-04-06T08:32:39.752148Z","shell.execute_reply":"2025-04-06T08:33:34.868858Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nLoading pretrained Word2Vec model...\nPretrained Word2Vec model loaded.\nGenerating weighted document embeddings for the training set...\nGenerating weighted document embeddings for the test set...\n\nTraining Weighted Word2Vec Embeddings Shape: (9051, 300)\nTest Weighted Word2Vec Embeddings Shape: (2263, 300)\n\nTraining Logistic Regression on Weighted Word2Vec embeddings...\n\nAccuracy with Weighted Word2Vec: 0.589041095890411\n\nClassification Report:\n                       precision    recall  f1-score   support\n\n             Atheism       0.39      0.29      0.34        96\n            Graphics       0.56      0.54      0.55       117\n             Windows       0.54      0.43      0.48       118\n         PC Hardware       0.50      0.48      0.49       118\n        Mac Hardware       0.50      0.44      0.47       115\n           X Windows       0.57      0.62      0.60       119\n            For Sale       0.62      0.60      0.61       117\n               Autos       0.38      0.63      0.47       119\n         Motorcycles       0.61      0.57      0.59       120\n            Baseball       0.67      0.73      0.70       119\n              Hockey       0.73      0.72      0.72       120\n        Cryptography       0.70      0.66      0.68       119\n         Electronics       0.61      0.60      0.61       118\n            Medicine       0.71      0.81      0.76       119\n               Space       0.71      0.65      0.68       119\n        Christianity       0.58      0.76      0.66       120\n                Guns       0.55      0.61      0.58       109\nMiddle East Politics       0.75      0.73      0.74       113\n    General Politics       0.61      0.59      0.60        93\n            Religion       0.21      0.05      0.09        75\n\n            accuracy                           0.59      2263\n           macro avg       0.58      0.58      0.57      2263\n        weighted avg       0.58      0.59      0.58      2263\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"","metadata":{}}]}